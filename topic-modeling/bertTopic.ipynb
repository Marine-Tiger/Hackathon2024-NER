{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import spacy\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.extract_corpus import get_corpus\n",
    "corpus = get_corpus('../data')\n",
    "nlp = spacy.load('fr_core_news_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTopic(language='french',  verbose=True)\n",
    "from typing import List\n",
    "from extract_corpus import Chunk\n",
    "\n",
    "\n",
    "texts = []\n",
    "\n",
    "def split_text(corpus:List[Chunk]):\n",
    "  current_text = \"\"\n",
    "  year_text = []\n",
    "  for chunk in corpus:\n",
    "    text = chunk.text + \".\"\n",
    "    if len(current_text) + len(text) < 512:\n",
    "        current_text += text\n",
    "    else:\n",
    "        year_text.append(current_text.strip())\n",
    "        current_text = text\n",
    "  if current_text:\n",
    "      year_text.append(current_text.strip())\n",
    "  return year_text\n",
    "\n",
    "# def remove_stop_words(text:str):\n",
    "#   return [token.text for token in nlp(text) if token.is_stop is False]\n",
    "\n",
    "# ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "# model = BERTopic(ctfidf_model=ctfidf_model, language=\"french\")\n",
    "#need to keep the stop words \n",
    "\n",
    "\n",
    "# for year in corpus:\n",
    "#   print(\"year\", year)\n",
    "#   text = [remove_stop_words(ytext) for ytext in split_text(corpus[year])]\n",
    "#   # text = \" \".join([token.text for token in nlp(text) if token.is_stop is False])\n",
    "#   texts.append(text)\n",
    "#   # text = \"\\n\".join([chunk.text for chunk in corpus['1814']])\n",
    "# # doc = nlp(\"\\n\".join([chunk.text for chunk in corpus['1814']]))\n",
    "# # text = \" \".join([tok.text for tok in doc if tok.is_stop == False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(topics):\n",
    "  response = {}\n",
    "  for topic in topics:\n",
    "    for item in model.get_topic(topic):\n",
    "      if item[0] not in response:\n",
    "        response[item[0]] = []\n",
    "      response[item[0]].append(item[1])\n",
    "  \n",
    "  for term in response:\n",
    "    response[term] = {\n",
    "      \"count\": len(response[term]),\n",
    "      \"max\": max(response[term]),\n",
    "      \"min\": min(response[term]),\n",
    "      \"average\": sum(response[term]) / len(response[term])\n",
    "    }\n",
    "  return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== year 1830\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "response = {}\n",
    "for year in [\"1830\"]:\n",
    "  try:\n",
    "    print(\"\\n\\n===== year\", year)\n",
    "    #split for each year and in each part of the text and remove stop words and join the text\n",
    "    texts = [\" \".join([tok.text for tok in nlp(text) if tok.is_stop is False]) for text in split_text(corpus[year])] \n",
    "    topics, probabilities = model.fit_transform(texts)\n",
    "    response[year] = extract_data(topics)\n",
    "    print(response[year])\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "# ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "# topic_model = BERTopic(ctfidf_model=ctfidf_model)\n",
    "\n",
    "# vectorizer_model = CountVectorizer(stop_words=\"french\")\n",
    "# model = BERTopic(vectorizer_model=vectorizer_model, language=\"french\")\n",
    "# clusters, probs = model.fit_transform(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in response:\n",
    "  print(year, list(response[year].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datamapplot\n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from bertopic import BERTopic\n",
    "# from umap import UMAP\n",
    "\n",
    "\n",
    "# sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# embeddings = sentence_model.encode(texts, show_progress_bar=False)\n",
    "\n",
    "# # Train BERTopic\n",
    "# topic_model = BERTopic().fit(texts, embeddings)\n",
    "\n",
    "# # # Reduce dimensionality of embeddings, this step is optional\n",
    "# reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "\n",
    "# # # Run the visualization with the original embeddings\n",
    "# topic_model.visualize_document_datamap(texts, embeddings=embeddings)\n",
    "\n",
    "# # # # Or, if you have reduced the original embeddings already:\n",
    "# # # topic_model.visualize_document_datamap(texts, reduced_embeddings=reduced_embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
