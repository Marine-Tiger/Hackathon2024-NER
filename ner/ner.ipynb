{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install crosslingual-coreference\n",
    "import spacy#, coreferee\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n",
    "# nlp.add_pipe('coreferee')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_lg')\n",
    "nlp.add_pipe(\n",
    "    \"xx_coref\", config={\"chunk_size\": 2500, \"chunk_overlap\": 2, \"device\": 0}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from extract_corpus import get_corpus\n",
    "\n",
    "corpus = get_corpus('../data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for year in corpus:\n",
    "#   print(year)\n",
    "\n",
    "for year in [\"1918\"]:\n",
    "# for year in corpus:\n",
    "  year_text = \"\\n\".join([chunk.text for chunk in corpus[year]]) #le but c'est de concatener le texte de chaque chunk de 1917\n",
    "  doc = nlp(year_text)\n",
    "  persons = [ent for ent in doc.ents if ent.label_ == \"PER\"]\n",
    "  print(f\"Year {year:<20}\")\n",
    "  for person in persons:\n",
    "    if (person.root.head.pos_ == \"ADJ\" and person.root.dep_==\"nsubj\"):\n",
    "      print(f\"{person.text:<20} {person.root.head.text:<20} {person.root.head.pos_:<10} {person.root.dep_:<10}\")\n",
    "  print(\"=================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_text = \"\\n\".join([chunk.text for chunk in corpus['1814']]) #le but c'est de concatener le texte de chaque chunk de 1917\n",
    "doc = nlp(year_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc =  nlp(\"Mme JOUDKIA est jolie. Elle mange des fleurs. Monsieur LEBORGNE est malade. Il est à l'hopital.\")\n",
    "\n",
    "adjectifs = [token for token in doc if token.pos_ == \"ADJ\"]\n",
    "for adj in adjectifs:\n",
    "  for child in adj.children:\n",
    "      if child.dep_ == \"nsubj\":\n",
    "        subj = None\n",
    "        for chunk in doc.noun_chunks:\n",
    "          if (chunk[0].i <= child.i) and (child.i <= chunk[-1].i):\n",
    "            subj = chunk\n",
    "        if not subj:\n",
    "          subj = child\n",
    "        print(f\"{adj.text:<20} || {subj.text:<20} || {subj.ents}\")\n",
    "        print(\"=================\")\n",
    "\n",
    "\n",
    "\n",
    "# persons = [ent for ent in doc.ents if ent.label_ == \"PER\"]\n",
    "# for person in persons:\n",
    "#   if (person.root.head.pos_ == \"ADJ\" and person.root.dep_==\"nsubj\"):\n",
    "#     print(f\"{person.text:<20} {person.root.head.text:<20} {person.root.head.pos_:<10} {person.root.dep_}\")\n",
    "# print(\"=================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.serve(doc, style=\"dep\")#, options={\"ents\": [\"PER\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(\"fr\")\n",
    "\n",
    "doc = nlp(\"Mme JOUDKIA est jolie. Elle mange des fleurs. Monsieur LEBORGNE est malade. Il est à l'hopital.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recursive_find_adjs(root, sent):\n",
    "    children = [w for w in sent.words if w.head == root.id]\n",
    "\n",
    "    if not children:\n",
    "        return []\n",
    "\n",
    "    filtered_c = [w for w in children if w.deprel == \"conj\" and w.upos == \"ADJ\"]\n",
    "    # Do not include an adjective if it is the parent of a noun to prevent\n",
    "    results = [w for w in filtered_c if not any(sub.head == w.id and sub.upos == \"NOUN\" for sub in sent.words)]\n",
    "    for w in children:\n",
    "        results += recursive_find_adjs(w, sent)\n",
    "\n",
    "    return results\n",
    "\n",
    "for sent in doc.sentences:\n",
    "    nouns = [w for w in sent.words if w.upos == \"NOUN\"]\n",
    "    # print(nouns[0].pretty_print())\n",
    "    # print(\"==\")\n",
    "    noun_adj_pairs = {}\n",
    "    for noun in nouns:\n",
    "        # Find constructions in the form of \"La voiture est belle\"\n",
    "        # In this scenario, the adjective is the parent of the noun\n",
    "        cop_root = sent.words[noun.head-1]\n",
    "        adjs = [cop_root] + recursive_find_adjs(cop_root, sent) if cop_root.upos == \"ADJ\" else []\n",
    "\n",
    "        # Find constructions in the form of \"La femme intelligente et belle\"\n",
    "        # Here, the adjectives are descendants of the noun\n",
    "        mod_adjs = [w for w in sent.words if w.head == noun.id and w.upos == \"ADJ\"]\n",
    "        # This should only be one element because conjunctions are hierarchical\n",
    "        if mod_adjs:\n",
    "            mod_adj = mod_adjs[0]\n",
    "            adjs.extend([mod_adj] + recursive_find_adjs(mod_adj, sent))\n",
    "\n",
    "        if adjs:\n",
    "            unique_adjs = []\n",
    "            unique_ids = set()\n",
    "            for adj in adjs:\n",
    "                if adj.id not in unique_ids:\n",
    "                    unique_adjs.append(adj)\n",
    "                    unique_ids.add(adj.id)\n",
    "\n",
    "            noun_adj_pairs[noun.text] = \" \".join([adj.text for adj in unique_adjs])\n",
    "\n",
    "    print(noun_adj_pairs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
