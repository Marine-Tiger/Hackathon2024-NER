{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from extract_corpus import get_coref_corpus\n",
    "\n",
    "# on utilise pour les adjectifs le corpus avec la coreference\n",
    "# car sinon on va trouver que par ex l'adjective \"gentille\" se rapporte a \"Elle\" dans la phrase\n",
    "# \"Mme Tiger est doctorante. Elle est gentille\".\n",
    "# ce qui n'est pas super utile\n",
    "# alors que le corpus coref a fait une passe sur toutes les phrases du dossier data pour transformer les phrases en \n",
    "# \"Mme Tiger est doctorante. Mme Tiger est gentille\".\n",
    "# et donc on trouvera que \"gentille\" se rapporte a \"Mme Tiger\"\n",
    "corpus = get_coref_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# petit script pour voir ce qui est contenu dans chaque token\n",
    "doc = nlp(\"Mme JOUDKIA est jolie. Elle mange des fleurs. Monsieur LEBORGNE est malade. Il est à l'hopital.\")\n",
    "for tok in doc:\n",
    "  print(f\"{tok.text:<10} {tok.dep_:<10} {tok.pos_} {tok.head}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_adjectives_subjects(doc):\n",
    "    # ici doc, correspond au texte d'une annee analyse par spacy \n",
    "    # cf: \n",
    "    # year_text = \"\\n\".join([chunk.text for chunk in corpus[year]]) )\n",
    "    # doc = nlp(year_text)\n",
    "\n",
    "\n",
    "    # cette fonction va remplir l'objet global_adjectives_subjects\n",
    "    # ca se passe en 2 parties\n",
    "    # 1) on va trouver tous les adjectivs et sujets pour ce doc (donc cette annee)\n",
    "    # 2) on va completer la variable global_adjectives_subjects qui contiendra tous les adjectivs et sujets cumules sur toutes les annees\n",
    "    \n",
    "\n",
    "    # 1) on instancie local_adj_subj : un objet qui s'occupera de contenir\n",
    "    # pour chaque sujet une liste de ses adjectifs POUR L'ANNEE EN COURS (pour le doc)\n",
    "    local_adj_subj = {}\n",
    "    # dans un premier temps, on cherche dans le doc (qui correspond au texte d'une annee) toutes les Entitees qui sont des personnes (Mme Tiger,...)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PER\":\n",
    "            # Si on trouve une entitee qui est une personne on dit que local_adj_subj[l'entitee] est un [] tableau vide (on a trouve un sujet, on trouvera ses adjectifs apres)\n",
    "            local_adj_subj[ent] = []\n",
    "            \n",
    "    # Donc a ce niveau on a rempli local_adj_subj de la facon suivante : {PERSONNE1: [], PERSONNE2: [], ...}\n",
    "    # ou PERSONNE 1, PERSONNE 2, sont du type Spacy.Span (https://spacy.io/api/span), ce qui veut dire qu'elles contiennent plusieurs tokens (ou un mais c'est quand meme une liste d'un element)\n",
    "\n",
    "    # maintenant on va boucler sur nos sujets (ent) et leus adjectifs associes (qui ici sont juste une liste vide : []) \n",
    "    # puis justement essayer de trouver les adjectifs\n",
    "    for ent, adjs in local_adj_subj.items():\n",
    "        # ici ent est donc une Span (une liste de tokens, ex [Mme, Tiger])\n",
    "        for token in ent:\n",
    "            # pour chaque token (Mme, puis Tiger) on va verifier que son head (ce a quoi il est ratache, n'est pas un ADJectif) (tu peux regarder l'attribut head dans https://spacy.io/api/token#attributes)\n",
    "            # et pour te familiariser avec la facon dont on cherche les infos avec spacy : https://spacy.io/usage/linguistic-features\n",
    "            if token.head.pos_ == \"ADJ\":\n",
    "                # si le head d'une partie du sujet est un ADJectif, on l'ajoute aux adjs(ectifs) du sujet\n",
    "                adjs.append(token.head)\n",
    "    \n",
    "    # Donc a ce niveau on a rempli local_adj_subj de la facon suivante : {PERSONNE1: [adj1, adj2,], PERSONNE2: [adj3,adj4], ...} \n",
    "    # pour l'annee actuelle (car on a rempli local_adj_subj et non pas l'objet global global_adjectives_subjects)\n",
    "\n",
    "    \n",
    "    # 2) on met à jour l'objet global global_adjectives_subjects qui contiendra tous les adjectifs et sujets cumulés sur TOUTES LES ANNÉES\n",
    "    # Pour chaque sujet / adjectifs trouves dans l'annee\n",
    "    for subject, adjs in local_adj_subj.items():\n",
    "        # Si le sujet n'existe pas encore dans le dictionnaire global (global_adjectives_subjects)\n",
    "        if subject.text not in global_adjectives_subjects:\n",
    "            # On crée une nouvelle entrée avec une liste vide\n",
    "            global_adjectives_subjects[subject.text] = []\n",
    "        # On ajoute les adjectifs de cette année à la liste existante\n",
    "        global_adjectives_subjects[subject.text].extend(adjs)\n",
    "\n",
    "\n",
    "# on defini un objet global_adjectives_subjects qui est ammene a contenir, \n",
    "# pour chaque sujet, la liste d'adjectifs associes sur toutes les annees:\n",
    "# ex  global_adjectives_subjects sera egal a ~ {\"Mme TIGER\": [\"gentille\", ...],}\n",
    "global_adjectives_subjects = {}\n",
    "\n",
    "\n",
    "# pour chaque annee du corpus de coref\n",
    "# on va recuperer le texte de l'annee\n",
    "# puis lancer la fonction compute_adjectives_subjects sur le doc spacy du texte \n",
    "for year in corpus:\n",
    "    print(\"YEAR = \", year)\n",
    "\n",
    "    #le but c'est de concatener le texte de chaque chunk de 1814\n",
    "    # comme dans les documents txt d'origine, on a ajoute une Chunk par ligne (cf process_document -> for line in text.split('\\n'):),\n",
    "    # pour retomber sur nos pattes, et avoir la meme structure de document que le texte d'origine, on joint les Chunk (lignes) par un retour a la ligne (\\n)\n",
    "    year_text = \"\\n\".join([chunk.text for chunk in corpus[year]]) #le but c'est de concatener le texte de chaque chunk de 1917\n",
    "\n",
    "    doc = nlp(year_text)\n",
    "    compute_adjectives_subjects(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silvain -> [excessif]\n",
      "M. -> [supérieur, prêts]\n",
      "M. Raphaël Duflos -> [douzième]\n",
      "Mme -> [première]\n",
      "M. de Max -> [relative]\n",
      "Mme Piérat -> [douzième, douzième]\n",
      "M. Mouché -> [agréable]\n",
      "M. Dessonnes -> [douzième, relatif, Française]\n",
      "M. Lucien Pallez -> [pendante]\n",
      "M. Fontaine -> [nouvelles]\n",
      "M. Grand -> [prêts]\n",
      "M. Brunot -> [douzième]\n",
      "M. Croué -> [douzième]\n",
      "M. Bernard -> [douzième]\n",
      "Mlle Delvair -> [douzième]\n",
      "Mme Silvain -> [douzième, relatives, relative, relative]\n",
      "Mlle Roch -> [douzième]\n",
      "Mlle Valpreux -> [prêts]\n",
      "M. Devred -> [droit]\n",
      "Elvire du Cid -> [tels, tels]\n",
      "Doris d’Iphigénie -> [inutile]\n",
      "Mme Lamorlette -> [impossible]\n",
      "M. Luguet -> [libre]\n",
      "M. P. Gavault -> [autre]\n",
      "Mlle Géniat -> [disponibles]\n",
      "G. Dubot -> [Dernière]\n",
      "M. Henry Roy -> [autre, autre]\n",
      "M. Albert Carré ancien -> [agréable]\n",
      "meai -> [premier]\n",
      "MM. Granval -> [relatives]\n"
     ]
    }
   ],
   "source": [
    "# ici simplement on affiche les sujets et leurs adjectifs\n",
    "for subject, adjs in global_adjectives_subjects.items():\n",
    "    if (len(adjs)>0):\n",
    "        print(f\"{subject} -> {adjs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1c4f7a20364884b16df33f554379c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', description='Subject:', placeholder='Filter subjects'), Text(value='', descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8cec42e223b4862a4f461a5ec4f7e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# une facon plus pratique de rechercher dans les resultats\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Create text widgets for filtering subjects and adjectives\n",
    "filter_subject = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Filter subjects',\n",
    "    description='Subject:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "filter_adj = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Filter adjectives',\n",
    "    description='Adjective:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create an Output widget to display the table\n",
    "table_output = widgets.Output()\n",
    "\n",
    "# Function to filter and display the table\n",
    "def filter_table(change):\n",
    "    with table_output:\n",
    "        table_output.clear_output(wait=True)\n",
    "        subject_text = filter_subject.value.lower()\n",
    "        adj_text = filter_adj.value.lower()\n",
    "        \n",
    "        rows = []\n",
    "        for subject, adjs in global_adjectives_subjects.items():\n",
    "            if subject_text in subject.lower():\n",
    "                matching_adjs = [adj for adj in adjs if adj_text in adj.text.lower()]\n",
    "                if matching_adjs:\n",
    "                    adj_string = ', '.join([adj.text for adj in matching_adjs])\n",
    "                    row = f\"<tr><td style='text-align:left;'><b>{subject}</b></td><td style='text-align:left;'>{adj_string}</td></tr>\"\n",
    "                    rows.append(row)\n",
    "        \n",
    "        table = \"<table>\" + \"\".join(rows) + \"</table>\"\n",
    "        display(HTML(table))\n",
    "\n",
    "# Display the filter widgets and create a container for the table\n",
    "display(widgets.HBox([filter_subject, filter_adj]))\n",
    "display(table_output)\n",
    "\n",
    "# Bind the filter function to both text widgets\n",
    "filter_subject.observe(filter_table, names='value')\n",
    "filter_adj.observe(filter_table, names='value')\n",
    "\n",
    "# Initial table display\n",
    "filter_table({'new': ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
